# Infrastructure

Inference engines, serving stacks, quantisation tools, vector databases, and deployment infrastructure for AI/LLM workloads.

## Contents

| Tool | What it does |
| :--- | :--- |
| [Ollama](../../services/ollama.md) | Local LLM inference server |
| [LiteLLM](../../services/litellm.md) | Unified LLM API proxy |
| [ZSE](zse.md) | Fast cold-start LLM inference engine |

<!-- New infrastructure pages are added here by Jules -->

## Sub-categories

- **Inference engines** — vLLM, TGI, llama.cpp, MLX, etc.
- **Vector databases** — Pinecone, Weaviate, Milvus, Qdrant, etc.
- **Serving & routing** — Load balancers, model routers, API gateways
- **Quantisation & optimisation** — GGUF, GPTQ, AWQ, etc.
