# SWE-bench

SWE-bench is a benchmark for evaluating LLMs on real-world software engineering tasks.

## Description
It uses actual issues from GitHub and requires the model to generate a functional patch that passes existing tests.

## Links
- [Official Website](https://www.swebench.com/)
- [GitHub Repository](https://github.com/princeton-nlp/SWE-bench)

## Alternatives
- [HumanEval](human-eval.md)
- [Terminal-Bench](terminal-bench.md)

## Backlog
- Track "Jules" performance on a subset of SWE-bench.

## Sources / References

- [Reference](https://www.swebench.com/)
- [Reference](https://github.com/princeton-nlp/SWE-bench)

## Contribution Metadata

- Last reviewed: 2026-02-26
- Confidence: medium
