# LM Evaluation Harness

A framework for few-shot evaluation of autoregressive language models.

## Description
It provides a unified interface for evaluating models on hundreds of different tasks, including MMLU, ARC, HellaSwag, and many more.

## Links
- [GitHub Repository](https://github.com/EleutherAI/lm-evaluation-harness)

## Alternatives
- [OpenCompass](https://opencompass.org.cn/)
- [HELM](https://crfm.stanford.edu/helm/lite/)

## Backlog
- Configure custom task suite for internal model evaluation.
