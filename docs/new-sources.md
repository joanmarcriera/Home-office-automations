# New Sources Staging

This file serves as a staging area for newly discovered AI tools, frameworks, papers, and articles before they are integrated into the core knowledge base.

| Title | URL | Date Discovered | Summary | Tags | Status |
|-------|-----|-----------------|---------|------|--------|
| MCP Registry | [https://modelcontextprotocol.info/tools/registry/](https://modelcontextprotocol.info/tools/registry/) | 2025-02-25 | Official central repository for publicly-available MCP servers. Provides a standardized way to discover and publish MCP servers using the `server.json` format. | tool, infrastructure, orchestration | integrated |
| RAGFlow | [https://github.com/infiniflow/ragflow](https://github.com/infiniflow/ragflow) | 2025-02-25 | Open-source RAG engine designed for deep document understanding. Integrates agentic capabilities and supports various document formats with intelligent chunking. | tool, framework, infrastructure | integrated |
| PageIndex | [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) | 2025-02-25 | Vectorless RAG framework using hierarchical tree indexing instead of vector similarity. Enables reasoning-based retrieval from long documents. | tool, framework, analysis | integrated |
| DREAM (Paper) | [arXiv:2602.18940](https://arxiv.org/abs/2602.18940) | 2025-02-25 | "Deep Research Evaluation with Agentic Metrics" proposes an agentic evaluation framework using tool-calling agents to assess temporal validity and factual correctness. | paper/article, benchmark/eval | integrated |
| LongCLI-Bench (Paper) | [arXiv:2602.14337](https://arxiv.org/abs/2602.14337) | 2025-02-25 | A benchmark for long-horizon agentic programming in command-line interfaces, evaluating agents on complex, multi-step programming tasks. | paper/article, benchmark/eval | integrated |
