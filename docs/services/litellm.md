# LiteLLM

LiteLLM is a proxy server that allows you to call 100+ LLMs using the OpenAI format.

## Description
It handles authentication, error handling, and cost tracking for multiple model providers.

## Links
- [Official Website](https://www.litellm.ai/)
- [GitHub Repository](https://github.com/BerriAI/litellm)

## Alternatives
- [One API](https://github.com/songquanpeng/one-api)
- [LocalAI](https://github.com/go-skynet/LocalAI)

## Backlog
- Configure budget alerts.
- Set up caching with Redis.
