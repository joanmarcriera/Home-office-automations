# Ollama

Ollama allows you to get up and running with large language models locally.

## Description
It provides a simple CLI and API for running models like Llama 3, Mistral, and others on your own hardware.

## Links
- [Official Website](https://ollama.com/)
- [GitHub Repository](https://github.com/ollama/ollama)

## Alternatives
- [LM Studio](https://lmstudio.ai/)
- [LocalAI](https://localai.io/)

## Backlog
- Benchmarking performance on TrueNAS SCALE.
- Setup GPU passthrough for faster inference.

## Sources / References

- [Reference](https://ollama.com/)
- [Reference](https://github.com/ollama/ollama)
- [Reference](https://lmstudio.ai/)

## Contribution Metadata

- Last reviewed: 2026-02-26
- Confidence: medium
